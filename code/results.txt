PS C:\Users\jaiog\OneDrive\rebecca uni\student_kit> pip install torch torchvision numpy tqdm scikit-learn
>> 
Collecting torch
  Downloading torch-2.10.0-cp314-cp314-win_amd64.whl.metadata (31 kB)
Collecting torchvision
  Downloading torchvision-0.25.0-cp314-cp314-win_amd64.whl.metadata (5.4 kB)
Collecting numpy
  Downloading numpy-2.4.1-cp314-cp314-win_amd64.whl.metadata (6.6 kB)
Collecting tqdm
  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting scikit-learn
  Downloading scikit_learn-1.8.0-cp314-cp314-win_amd64.whl.metadata (11 kB)
Collecting filelock (from torch)
  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)
Collecting typing-extensions>=4.10.0 (from torch)
  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting sympy>=1.13.3 (from torch)
  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx>=2.5.1 (from torch)
  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)
Collecting jinja2 (from torch)
  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting fsspec>=0.8.5 (from torch)
  Downloading fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)
Collecting setuptools (from torch)
  Downloading setuptools-80.10.1-py3-none-any.whl.metadata (6.7 kB)
Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)
  Downloading pillow-12.1.0-cp314-cp314-win_amd64.whl.metadata (9.0 kB)
Collecting colorama (from tqdm)
  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)
Collecting scipy>=1.10.0 (from scikit-learn)
  Downloading scipy-1.17.0-cp314-cp314-win_amd64.whl.metadata (60 kB)
Collecting joblib>=1.3.0 (from scikit-learn)
  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)
Collecting threadpoolctl>=3.2.0 (from scikit-learn)
  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)
  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch)
  Downloading markupsafe-3.0.3-cp314-cp314-win_amd64.whl.metadata (2.8 kB)
Downloading torch-2.10.0-cp314-cp314-win_amd64.whl (113.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.8/113.8 MB 15.9 MB/s  0:00:07
Downloading torchvision-0.25.0-cp314-cp314-win_amd64.whl (4.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 16.4 MB/s  0:00:00
Downloading numpy-2.4.1-cp314-cp314-win_amd64.whl (12.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/12.4 MB 17.2 MB/s  0:00:00
Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)
Downloading scikit_learn-1.8.0-cp314-cp314-win_amd64.whl (8.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.1/8.1 MB 16.3 MB/s  0:00:00
Downloading fsspec-2026.1.0-py3-none-any.whl (201 kB)
Downloading joblib-1.5.3-py3-none-any.whl (309 kB)
Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 15.5 MB/s  0:00:00
Downloading pillow-12.1.0-cp314-cp314-win_amd64.whl (7.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 17.2 MB/s  0:00:00
Downloading scipy-1.17.0-cp314-cp314-win_amd64.whl (37.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.1/37.1 MB 16.9 MB/s  0:00:02
Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 12.5 MB/s  0:00:00
Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 10.0 MB/s  0:00:00
Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)
Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Downloading filelock-3.20.3-py3-none-any.whl (16 kB)
Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)
Downloading markupsafe-3.0.3-cp314-cp314-win_amd64.whl (15 kB)
Downloading setuptools-80.10.1-py3-none-any.whl (1.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 6.0 MB/s  0:00:00
Installing collected packages: mpmath, typing-extensions, threadpoolctl, sympy, setuptools, pillow, numpy, networkx, MarkupSafe, joblib, fsspec, filelock, colorama, tqdm, scipy, jinja2, torch, scikit-learn, torchvision
Successfully installed MarkupSafe-3.0.3 colorama-0.4.6 filelock-3.20.3 fsspec-2026.1.0 jinja2-3.1.6 joblib-1.5.3 mpmath-1.3.0 networkx-3.6.1 numpy-2.4.1 pillow-12.1.0 scikit-learn-1.8.0 scipy-1.17.0 setuptools-80.10.1 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.10.0 torchvision-0.25.0 tqdm-4.67.1 typing-extensions-4.15.0

[notice] A new release of pip is available: 25.2 -> 25.3
[notice] To update, run: C:\Users\jaiog\AppData\Local\Programs\Python\Python314\python.exe -m pip install --upgrade pip
PS C:\Users\jaiog\OneDrive\rebecca uni\student_kit> python train_shadows.py
>>
✓ Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU

============================================================
Training Shadow Models for Membership Inference Attack
============================================================
Files already downloaded and verified
Files already downloaded and verified

============================================================
Shadow Model 1/3
============================================================
Training samples: 1500

Training Shadow Model 1...
Epoch 1/120: 100%|███████████████████████████| 47/47 [00:02<00:00, 22.87it/s, Loss=2.487, Acc=21.7%]
Epoch 2/120: 100%|███████████████████████████| 47/47 [00:00<00:00, 74.94it/s, Loss=2.286, Acc=26.7%]
Epoch 3/120: 100%|███████████████████████████| 47/47 [00:00<00:00, 48.66it/s, Loss=1.982, Acc=31.0%]
Epoch 4/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 46.05it/s, Loss=1.890, Acc=34.9%]
Epoch 5/120: 100%|███████████████████████████| 47/47 [00:00<00:00, 55.30it/s, Loss=2.215, Acc=29.5%]
Epoch 6/120: 100%|███████████████████████████| 47/47 [00:00<00:00, 48.08it/s, Loss=1.988, Acc=32.5%]
Epoch 7/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 45.02it/s, Loss=1.820, Acc=36.9%]
Epoch 8/120: 100%|███████████████████████████| 47/47 [00:00<00:00, 52.65it/s, Loss=1.640, Acc=46.4%]
Epoch 9/120: 100%|███████████████████████████| 47/47 [00:00<00:00, 52.01it/s, Loss=1.530, Acc=51.5%]
Epoch 10/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 47.81it/s, Loss=1.444, Acc=53.7%]
  Epoch 10: Train 53.73% | Test 32.03% | Gap 21.70%
Epoch 11/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.61it/s, Loss=1.236, Acc=62.5%]
Epoch 12/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.02it/s, Loss=1.079, Acc=69.0%]
Epoch 13/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.51it/s, Loss=1.292, Acc=62.9%]
Epoch 14/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 53.33it/s, Loss=0.971, Acc=70.6%]
Epoch 15/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.95it/s, Loss=0.790, Acc=76.9%]
Epoch 16/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.93it/s, Loss=0.947, Acc=75.1%]
Epoch 17/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.33it/s, Loss=1.009, Acc=71.4%]
Epoch 18/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.85it/s, Loss=0.961, Acc=74.3%]
Epoch 19/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.07it/s, Loss=0.821, Acc=78.5%]
Epoch 20/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 54.47it/s, Loss=1.255, Acc=71.3%]
  Epoch 20: Train 71.33% | Test 25.66% | Gap 45.67%
Epoch 21/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 56.56it/s, Loss=1.273, Acc=67.3%]
Epoch 22/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 55.64it/s, Loss=0.954, Acc=77.2%]
Epoch 23/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 68.05it/s, Loss=1.376, Acc=70.5%]
Epoch 24/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 65.15it/s, Loss=1.210, Acc=73.0%]
Epoch 25/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 46.15it/s, Loss=1.281, Acc=73.2%]
Epoch 26/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.16it/s, Loss=1.619, Acc=63.3%]
Epoch 27/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.88it/s, Loss=1.550, Acc=60.8%]
Epoch 28/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.44it/s, Loss=1.427, Acc=62.1%]
Epoch 29/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 53.25it/s, Loss=1.017, Acc=74.2%]
Epoch 30/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 44.59it/s, Loss=0.859, Acc=78.5%]
  Epoch 30: Train 78.47% | Test 26.91% | Gap 51.56%
Epoch 31/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.96it/s, Loss=0.506, Acc=87.3%]
Epoch 32/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 47.74it/s, Loss=0.230, Acc=93.7%]
Epoch 33/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 53.62it/s, Loss=0.170, Acc=95.3%]
Epoch 34/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 55.19it/s, Loss=0.092, Acc=97.2%]
Epoch 35/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.86it/s, Loss=0.066, Acc=97.5%]
Epoch 36/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 50.06it/s, Loss=0.049, Acc=98.3%]
Epoch 37/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.74it/s, Loss=0.044, Acc=98.5%]
Epoch 38/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.20it/s, Loss=0.042, Acc=98.7%]
Epoch 39/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.01it/s, Loss=0.039, Acc=98.8%]
Epoch 40/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 50.36it/s, Loss=0.033, Acc=98.9%]
  Epoch 40: Train 98.93% | Test 29.01% | Gap 69.92%
Epoch 41/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 55.78it/s, Loss=0.031, Acc=98.9%]
Epoch 42/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.01it/s, Loss=0.029, Acc=99.0%]
Epoch 43/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.80it/s, Loss=0.030, Acc=99.0%]
Epoch 44/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 47.54it/s, Loss=0.027, Acc=99.0%]
Epoch 45/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 45.75it/s, Loss=0.026, Acc=99.0%]
Epoch 46/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 47.73it/s, Loss=0.024, Acc=99.1%]
Epoch 47/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 43.29it/s, Loss=0.028, Acc=98.9%]
Epoch 48/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 44.28it/s, Loss=0.028, Acc=99.0%]
Epoch 49/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 45.87it/s, Loss=0.025, Acc=99.0%]
Epoch 50/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 50.18it/s, Loss=0.025, Acc=99.0%]
  Epoch 50: Train 99.00% | Test 28.72% | Gap 70.28%
Epoch 51/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 45.17it/s, Loss=0.025, Acc=99.0%]
Epoch 52/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.51it/s, Loss=0.024, Acc=99.0%]
Epoch 53/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 47.04it/s, Loss=0.023, Acc=99.1%]
Epoch 54/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.41it/s, Loss=0.024, Acc=99.1%]
Epoch 55/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.09it/s, Loss=0.022, Acc=99.1%]
Epoch 56/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 50.15it/s, Loss=0.023, Acc=99.1%]
Epoch 57/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 45.81it/s, Loss=0.023, Acc=99.1%]
Epoch 58/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.45it/s, Loss=0.022, Acc=99.1%]
Epoch 59/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 57.50it/s, Loss=0.021, Acc=99.1%]
Epoch 60/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.36it/s, Loss=0.021, Acc=99.1%]
  Epoch 60: Train 99.07% | Test 28.64% | Gap 70.43%
Epoch 61/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 46.49it/s, Loss=0.021, Acc=99.1%]
Epoch 62/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.02it/s, Loss=0.022, Acc=99.1%]
Epoch 63/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.83it/s, Loss=0.022, Acc=99.2%]
Epoch 64/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.20it/s, Loss=0.021, Acc=99.3%]
Epoch 65/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 55.92it/s, Loss=0.022, Acc=99.3%]
Epoch 66/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.05it/s, Loss=0.021, Acc=99.3%]
Epoch 67/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.21it/s, Loss=0.020, Acc=99.3%]
Epoch 68/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 46.45it/s, Loss=0.019, Acc=99.3%]
Epoch 69/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.82it/s, Loss=0.020, Acc=99.3%]
Epoch 70/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 43.73it/s, Loss=0.019, Acc=99.3%]
  Epoch 70: Train 99.27% | Test 28.69% | Gap 70.58%
Epoch 71/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 48.24it/s, Loss=0.020, Acc=99.3%]
Epoch 72/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 46.92it/s, Loss=0.018, Acc=99.3%]
Epoch 73/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 46.53it/s, Loss=0.020, Acc=99.3%]
Epoch 74/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.42it/s, Loss=0.019, Acc=99.3%]
Epoch 75/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 57.19it/s, Loss=0.019, Acc=99.3%]
Epoch 76/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.01it/s, Loss=0.020, Acc=99.3%]
Epoch 77/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 50.59it/s, Loss=0.020, Acc=99.3%]
Epoch 78/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 56.23it/s, Loss=0.020, Acc=99.3%]
Epoch 79/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.84it/s, Loss=0.020, Acc=99.3%]
Epoch 80/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.05it/s, Loss=0.019, Acc=99.3%]
  Epoch 80: Train 99.27% | Test 28.80% | Gap 70.47%
Epoch 81/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 57.03it/s, Loss=0.019, Acc=99.3%]
Epoch 82/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.25it/s, Loss=0.019, Acc=99.3%]
Epoch 83/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 42.71it/s, Loss=0.018, Acc=99.3%]
Epoch 84/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 57.52it/s, Loss=0.018, Acc=99.3%]
Epoch 85/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.52it/s, Loss=0.019, Acc=99.4%]
Epoch 86/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 54.79it/s, Loss=0.019, Acc=99.4%]
Epoch 87/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 46.20it/s, Loss=0.019, Acc=99.4%]
Epoch 88/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 54.35it/s, Loss=0.019, Acc=99.4%]
Epoch 89/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.81it/s, Loss=0.019, Acc=99.4%]
Epoch 90/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 51.95it/s, Loss=0.018, Acc=99.4%]
  Epoch 90: Train 99.40% | Test 28.85% | Gap 70.55%
Epoch 91/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.09it/s, Loss=0.018, Acc=99.4%]
Epoch 92/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 53.74it/s, Loss=0.019, Acc=99.4%]
Epoch 93/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 50.36it/s, Loss=0.017, Acc=99.4%]
Epoch 94/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.40it/s, Loss=0.018, Acc=99.4%]
Epoch 95/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 54.57it/s, Loss=0.018, Acc=99.4%]
Epoch 96/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 55.70it/s, Loss=0.018, Acc=99.4%]
Epoch 97/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 52.18it/s, Loss=0.018, Acc=99.4%]
Epoch 98/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 56.24it/s, Loss=0.018, Acc=99.4%]
Epoch 99/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 49.43it/s, Loss=0.017, Acc=99.4%]
Epoch 100/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 51.35it/s, Loss=0.018, Acc=99.4%]
  Epoch 100: Train 99.40% | Test 28.80% | Gap 70.60%
Epoch 101/120: 100%|█████████████████████████| 47/47 [00:01<00:00, 46.77it/s, Loss=0.017, Acc=99.4%]
Epoch 102/120: 100%|█████████████████████████| 47/47 [00:01<00:00, 46.63it/s, Loss=0.017, Acc=99.4%]
Epoch 103/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 54.48it/s, Loss=0.017, Acc=99.4%]
Epoch 104/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 54.20it/s, Loss=0.018, Acc=99.4%]
Epoch 105/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 47.66it/s, Loss=0.018, Acc=99.4%]
Epoch 106/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 50.01it/s, Loss=0.017, Acc=99.4%]
Epoch 107/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 48.01it/s, Loss=0.017, Acc=99.4%]
Epoch 108/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 50.97it/s, Loss=0.018, Acc=99.4%]
Epoch 109/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 48.86it/s, Loss=0.018, Acc=99.4%]
Epoch 110/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 47.39it/s, Loss=0.018, Acc=99.4%]
  Epoch 110: Train 99.40% | Test 28.85% | Gap 70.55%
Epoch 111/120: 100%|█████████████████████████| 47/47 [00:01<00:00, 46.53it/s, Loss=0.016, Acc=99.4%]
Epoch 112/120: 100%|█████████████████████████| 47/47 [00:01<00:00, 46.97it/s, Loss=0.018, Acc=99.4%]
Epoch 113/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 53.10it/s, Loss=0.018, Acc=99.4%]
Epoch 114/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 50.71it/s, Loss=0.018, Acc=99.4%]
Epoch 115/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 51.98it/s, Loss=0.017, Acc=99.4%]
Epoch 116/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 54.15it/s, Loss=0.018, Acc=99.4%]
Epoch 117/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 53.52it/s, Loss=0.018, Acc=99.4%]
Epoch 118/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 56.04it/s, Loss=0.017, Acc=99.4%]
Epoch 119/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 51.04it/s, Loss=0.017, Acc=99.4%]
Epoch 120/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 56.01it/s, Loss=0.018, Acc=99.4%]
  Epoch 120: Train 99.40% | Test 28.83% | Gap 70.57%
✓ Saved to checkpoints/shadow/shadow_1.pth


============================================================
Shadow Model 2/3
============================================================
Training samples: 1500

Training Shadow Model 2...
Epoch 1/120: 100%|███████████████████████████| 47/47 [00:00<00:00, 55.79it/s, Loss=2.345, Acc=19.0%]
Epoch 2/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 30.10it/s, Loss=2.049, Acc=24.9%]
Epoch 3/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 35.29it/s, Loss=2.176, Acc=26.8%]
Epoch 4/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 32.76it/s, Loss=2.050, Acc=25.5%]
Epoch 5/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 35.55it/s, Loss=2.116, Acc=25.5%]
Epoch 6/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 38.10it/s, Loss=2.096, Acc=29.5%]
Epoch 7/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 35.21it/s, Loss=1.815, Acc=35.1%]
Epoch 8/120: 100%|███████████████████████████| 47/47 [00:01<00:00, 36.74it/s, Loss=1.846, Acc=37.9%]
Epoch 9/120: 100%|███████████████████████████| 47/47 [00:02<00:00, 22.90it/s, Loss=1.734, Acc=39.3%]
Epoch 10/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 30.13it/s, Loss=1.640, Acc=46.4%]
  Epoch 10: Train 46.40% | Test 30.97% | Gap 15.43%
Epoch 11/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 37.28it/s, Loss=1.499, Acc=49.7%]
Epoch 12/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 38.71it/s, Loss=1.035, Acc=64.9%]
Epoch 13/120: 100%|██████████████████████████| 47/47 [00:01<00:00, 34.92it/s, Loss=0.884, Acc=73.5%]
Epoch 14/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 69.33it/s, Loss=0.826, Acc=74.1%]
Epoch 15/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 85.39it/s, Loss=0.837, Acc=72.5%]
Epoch 16/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 56.62it/s, Loss=1.003, Acc=72.4%]
Epoch 17/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 122.70it/s, Loss=1.158, Acc=71.9%]
Epoch 18/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 143.13it/s, Loss=0.835, Acc=75.4%]
Epoch 19/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 123.29it/s, Loss=0.848, Acc=80.1%]
Epoch 20/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 120.52it/s, Loss=1.518, Acc=68.1%]
  Epoch 20: Train 68.13% | Test 27.22% | Gap 40.91%
Epoch 21/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 122.41it/s, Loss=1.164, Acc=70.4%]
Epoch 22/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 121.51it/s, Loss=1.063, Acc=73.3%]
Epoch 23/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 107.06it/s, Loss=0.800, Acc=78.5%]
Epoch 24/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 117.91it/s, Loss=1.038, Acc=79.0%]
Epoch 25/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 117.21it/s, Loss=1.302, Acc=72.6%]
Epoch 26/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 116.82it/s, Loss=1.081, Acc=77.3%]
Epoch 27/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 116.05it/s, Loss=1.215, Acc=75.9%]
Epoch 28/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 105.71it/s, Loss=1.129, Acc=73.5%]
Epoch 29/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 117.05it/s, Loss=1.391, Acc=70.1%]
Epoch 30/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 101.61it/s, Loss=0.777, Acc=79.8%]
  Epoch 30: Train 79.80% | Test 25.43% | Gap 54.37%
Epoch 31/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 114.28it/s, Loss=0.423, Acc=90.0%]
Epoch 32/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 112.40it/s, Loss=0.198, Acc=95.4%]
Epoch 33/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 108.28it/s, Loss=0.089, Acc=97.6%]
Epoch 34/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 110.97it/s, Loss=0.084, Acc=98.4%]
Epoch 35/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 115.20it/s, Loss=0.069, Acc=98.5%]
Epoch 36/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 72.01it/s, Loss=0.049, Acc=98.7%]
Epoch 37/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 148.12it/s, Loss=0.064, Acc=98.9%]
Epoch 38/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 124.29it/s, Loss=0.047, Acc=98.9%]
Epoch 39/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 115.94it/s, Loss=0.053, Acc=99.0%]
Epoch 40/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 154.39it/s, Loss=0.053, Acc=99.1%]
  Epoch 40: Train 99.13% | Test 28.42% | Gap 70.71%
Epoch 41/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 143.28it/s, Loss=0.037, Acc=99.1%]
Epoch 42/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 160.29it/s, Loss=0.046, Acc=99.1%]
Epoch 43/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.29it/s, Loss=0.037, Acc=99.1%]
Epoch 44/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.62it/s, Loss=0.036, Acc=99.1%]
Epoch 45/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 119.36it/s, Loss=0.037, Acc=99.1%]
Epoch 46/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 120.50it/s, Loss=0.036, Acc=99.1%]
Epoch 47/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 125.96it/s, Loss=0.036, Acc=99.1%]
Epoch 48/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 128.71it/s, Loss=0.033, Acc=99.1%]
Epoch 49/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 131.20it/s, Loss=0.033, Acc=99.1%]
Epoch 50/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 123.82it/s, Loss=0.034, Acc=99.1%]
  Epoch 50: Train 99.13% | Test 29.05% | Gap 70.08%
Epoch 51/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 120.05it/s, Loss=0.035, Acc=99.1%]
Epoch 52/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 121.43it/s, Loss=0.033, Acc=99.1%]
Epoch 53/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 123.05it/s, Loss=0.032, Acc=99.1%]
Epoch 54/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 125.66it/s, Loss=0.032, Acc=99.1%]
Epoch 55/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 131.29it/s, Loss=0.030, Acc=99.1%]
Epoch 56/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.93it/s, Loss=0.031, Acc=99.1%]
Epoch 57/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 121.74it/s, Loss=0.032, Acc=99.1%]
Epoch 58/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 121.51it/s, Loss=0.031, Acc=99.1%]
Epoch 59/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 122.70it/s, Loss=0.031, Acc=99.1%]
Epoch 60/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 118.47it/s, Loss=0.034, Acc=99.1%]
  Epoch 60: Train 99.13% | Test 29.12% | Gap 70.01%
Epoch 61/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 120.58it/s, Loss=0.032, Acc=99.1%]
Epoch 62/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 128.21it/s, Loss=0.029, Acc=99.1%]
Epoch 63/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.77it/s, Loss=0.029, Acc=99.1%]
Epoch 64/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.03it/s, Loss=0.031, Acc=99.1%]
Epoch 65/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.09it/s, Loss=0.029, Acc=99.1%]
Epoch 66/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 124.68it/s, Loss=0.030, Acc=99.1%]
Epoch 67/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 115.16it/s, Loss=0.033, Acc=99.1%]
Epoch 68/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 122.33it/s, Loss=0.031, Acc=99.1%]
Epoch 69/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.38it/s, Loss=0.028, Acc=99.1%]
Epoch 70/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.04it/s, Loss=0.028, Acc=99.1%]
  Epoch 70: Train 99.13% | Test 29.22% | Gap 69.91%
Epoch 71/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 119.55it/s, Loss=0.031, Acc=99.1%]
Epoch 72/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 106.89it/s, Loss=0.026, Acc=99.1%]
Epoch 73/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 120.73it/s, Loss=0.031, Acc=99.1%]
Epoch 74/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.14it/s, Loss=0.029, Acc=99.1%]
Epoch 75/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.83it/s, Loss=0.028, Acc=99.1%]
Epoch 76/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.37it/s, Loss=0.028, Acc=99.1%]
Epoch 77/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 126.98it/s, Loss=0.030, Acc=99.2%]
Epoch 78/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.26it/s, Loss=0.027, Acc=99.2%]
Epoch 79/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.02it/s, Loss=0.028, Acc=99.2%]
Epoch 80/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.46it/s, Loss=0.027, Acc=99.2%]
  Epoch 80: Train 99.20% | Test 29.25% | Gap 69.95%
Epoch 81/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 122.34it/s, Loss=0.029, Acc=99.2%]
Epoch 82/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.77it/s, Loss=0.028, Acc=99.2%]
Epoch 83/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.91it/s, Loss=0.028, Acc=99.2%]
Epoch 84/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.09it/s, Loss=0.029, Acc=99.2%]
Epoch 85/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.68it/s, Loss=0.029, Acc=99.2%]
Epoch 86/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 122.15it/s, Loss=0.029, Acc=99.2%]
Epoch 87/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 123.65it/s, Loss=0.029, Acc=99.2%]
Epoch 88/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.55it/s, Loss=0.028, Acc=99.2%]
Epoch 89/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.79it/s, Loss=0.028, Acc=99.2%]
Epoch 90/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 66.83it/s, Loss=0.026, Acc=99.2%]
  Epoch 90: Train 99.20% | Test 29.31% | Gap 69.89%
Epoch 91/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 71.15it/s, Loss=0.029, Acc=99.2%]
Epoch 92/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 115.18it/s, Loss=0.031, Acc=99.2%]
Epoch 93/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 98.84it/s, Loss=0.028, Acc=99.2%]
Epoch 94/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 124.64it/s, Loss=0.029, Acc=99.2%]
Epoch 95/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 117.46it/s, Loss=0.027, Acc=99.2%]
Epoch 96/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 108.23it/s, Loss=0.024, Acc=99.2%]
Epoch 97/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.07it/s, Loss=0.028, Acc=99.2%]
Epoch 98/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 119.09it/s, Loss=0.028, Acc=99.2%]
Epoch 99/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 103.91it/s, Loss=0.026, Acc=99.2%]
Epoch 100/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 73.23it/s, Loss=0.029, Acc=99.2%]
  Epoch 100: Train 99.20% | Test 29.29% | Gap 69.91%
Epoch 101/120: 100%|████████████████████████| 47/47 [00:00<00:00, 119.61it/s, Loss=0.030, Acc=99.2%]
Epoch 102/120: 100%|████████████████████████| 47/47 [00:00<00:00, 129.07it/s, Loss=0.028, Acc=99.2%]
Epoch 103/120: 100%|████████████████████████| 47/47 [00:00<00:00, 122.92it/s, Loss=0.029, Acc=99.2%]
Epoch 104/120: 100%|████████████████████████| 47/47 [00:00<00:00, 111.93it/s, Loss=0.031, Acc=99.2%]
Epoch 105/120: 100%|████████████████████████| 47/47 [00:00<00:00, 111.72it/s, Loss=0.031, Acc=99.2%]
Epoch 106/120: 100%|████████████████████████| 47/47 [00:00<00:00, 124.64it/s, Loss=0.028, Acc=99.2%]
Epoch 107/120: 100%|████████████████████████| 47/47 [00:00<00:00, 128.54it/s, Loss=0.028, Acc=99.2%]
Epoch 108/120: 100%|████████████████████████| 47/47 [00:00<00:00, 130.10it/s, Loss=0.028, Acc=99.2%]
Epoch 109/120: 100%|████████████████████████| 47/47 [00:00<00:00, 132.99it/s, Loss=0.026, Acc=99.2%]
Epoch 110/120: 100%|████████████████████████| 47/47 [00:00<00:00, 124.41it/s, Loss=0.028, Acc=99.2%]
  Epoch 110: Train 99.20% | Test 29.29% | Gap 69.91%
Epoch 111/120: 100%|████████████████████████| 47/47 [00:00<00:00, 129.01it/s, Loss=0.027, Acc=99.2%]
Epoch 112/120: 100%|████████████████████████| 47/47 [00:00<00:00, 130.17it/s, Loss=0.028, Acc=99.2%]
Epoch 113/120: 100%|████████████████████████| 47/47 [00:00<00:00, 130.69it/s, Loss=0.026, Acc=99.2%]
Epoch 114/120: 100%|████████████████████████| 47/47 [00:00<00:00, 138.46it/s, Loss=0.026, Acc=99.2%]
Epoch 115/120: 100%|████████████████████████| 47/47 [00:00<00:00, 136.06it/s, Loss=0.026, Acc=99.2%]
Epoch 116/120: 100%|████████████████████████| 47/47 [00:00<00:00, 129.76it/s, Loss=0.028, Acc=99.2%]
Epoch 117/120: 100%|████████████████████████| 47/47 [00:00<00:00, 127.76it/s, Loss=0.028, Acc=99.2%]
Epoch 118/120: 100%|████████████████████████| 47/47 [00:00<00:00, 133.73it/s, Loss=0.027, Acc=99.2%]
Epoch 119/120: 100%|████████████████████████| 47/47 [00:00<00:00, 141.21it/s, Loss=0.025, Acc=99.2%]
Epoch 120/120: 100%|████████████████████████| 47/47 [00:00<00:00, 142.82it/s, Loss=0.025, Acc=99.2%]
  Epoch 120: Train 99.20% | Test 29.41% | Gap 69.79%
✓ Saved to checkpoints/shadow/shadow_2.pth


============================================================
Shadow Model 3/3
============================================================
Training samples: 1500

Training Shadow Model 3...
Epoch 1/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 134.31it/s, Loss=2.348, Acc=20.1%]
Epoch 2/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 122.02it/s, Loss=2.275, Acc=28.1%]
Epoch 3/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 124.38it/s, Loss=2.203, Acc=33.1%]
Epoch 4/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 122.86it/s, Loss=2.315, Acc=32.4%]
Epoch 5/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 122.75it/s, Loss=2.022, Acc=37.5%]
Epoch 6/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 126.14it/s, Loss=2.096, Acc=36.2%]
Epoch 7/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 132.69it/s, Loss=1.784, Acc=41.7%]
Epoch 8/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 136.82it/s, Loss=1.586, Acc=47.4%]
Epoch 9/120: 100%|██████████████████████████| 47/47 [00:00<00:00, 134.99it/s, Loss=1.546, Acc=52.1%]
Epoch 10/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.95it/s, Loss=1.286, Acc=57.3%]
  Epoch 10: Train 57.27% | Test 31.34% | Gap 25.93%
Epoch 11/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.72it/s, Loss=1.404, Acc=57.2%]
Epoch 12/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.86it/s, Loss=1.295, Acc=62.7%]
Epoch 13/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.10it/s, Loss=1.126, Acc=69.3%]
Epoch 14/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 118.37it/s, Loss=1.264, Acc=68.5%]
Epoch 15/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.30it/s, Loss=1.418, Acc=63.7%]
Epoch 16/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.68it/s, Loss=1.091, Acc=71.1%]
Epoch 17/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 122.55it/s, Loss=1.674, Acc=59.5%]
Epoch 18/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 118.82it/s, Loss=1.649, Acc=63.0%]
Epoch 19/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.36it/s, Loss=1.427, Acc=65.1%]
Epoch 20/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.32it/s, Loss=1.291, Acc=68.4%]
  Epoch 20: Train 68.40% | Test 24.12% | Gap 44.28%
Epoch 21/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.05it/s, Loss=1.116, Acc=73.6%]
Epoch 22/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.59it/s, Loss=1.637, Acc=57.7%]
Epoch 23/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.24it/s, Loss=1.449, Acc=66.9%]
Epoch 24/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.51it/s, Loss=1.888, Acc=56.7%]
Epoch 25/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 138.19it/s, Loss=1.738, Acc=56.0%]
Epoch 26/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.75it/s, Loss=1.196, Acc=68.1%]
Epoch 27/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.40it/s, Loss=1.334, Acc=70.6%]
Epoch 28/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 131.24it/s, Loss=1.589, Acc=66.3%]
Epoch 29/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.98it/s, Loss=2.267, Acc=52.0%]
Epoch 30/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.30it/s, Loss=2.116, Acc=47.5%]
  Epoch 30: Train 47.53% | Test 19.86% | Gap 27.67%
Epoch 31/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 124.01it/s, Loss=1.517, Acc=59.9%]
Epoch 32/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.43it/s, Loss=0.824, Acc=78.1%]
Epoch 33/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.29it/s, Loss=0.489, Acc=84.3%]
Epoch 34/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.47it/s, Loss=0.343, Acc=88.5%]
Epoch 35/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 125.70it/s, Loss=0.287, Acc=90.5%]
Epoch 36/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.57it/s, Loss=0.237, Acc=92.4%]
Epoch 37/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.33it/s, Loss=0.186, Acc=93.7%]
Epoch 38/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.88it/s, Loss=0.185, Acc=94.2%]
Epoch 39/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.25it/s, Loss=0.160, Acc=94.5%]
Epoch 40/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 139.86it/s, Loss=0.136, Acc=95.1%]
  Epoch 40: Train 95.13% | Test 27.92% | Gap 67.21%
Epoch 41/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 128.21it/s, Loss=0.124, Acc=95.9%]
Epoch 42/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.81it/s, Loss=0.107, Acc=96.7%]
Epoch 43/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.11it/s, Loss=0.095, Acc=97.2%]
Epoch 44/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.42it/s, Loss=0.077, Acc=97.6%]
Epoch 45/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 125.81it/s, Loss=0.070, Acc=97.8%]
Epoch 46/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 131.07it/s, Loss=0.059, Acc=98.2%]
Epoch 47/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.56it/s, Loss=0.053, Acc=98.3%]
Epoch 48/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.98it/s, Loss=0.054, Acc=98.2%]
Epoch 49/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.03it/s, Loss=0.049, Acc=98.5%]
Epoch 50/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 123.91it/s, Loss=0.050, Acc=98.6%]
  Epoch 50: Train 98.60% | Test 27.70% | Gap 70.90%
Epoch 51/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.80it/s, Loss=0.045, Acc=98.7%]
Epoch 52/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.47it/s, Loss=0.042, Acc=98.7%]
Epoch 53/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.32it/s, Loss=0.041, Acc=98.7%]
Epoch 54/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 138.23it/s, Loss=0.037, Acc=98.5%]
Epoch 55/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.89it/s, Loss=0.038, Acc=98.7%]
Epoch 56/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 131.82it/s, Loss=0.038, Acc=98.7%]
Epoch 57/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 126.39it/s, Loss=0.037, Acc=98.9%]
Epoch 58/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.63it/s, Loss=0.035, Acc=98.8%]
Epoch 59/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.14it/s, Loss=0.032, Acc=99.1%]
Epoch 60/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 138.30it/s, Loss=0.031, Acc=99.0%]
  Epoch 60: Train 99.00% | Test 27.83% | Gap 71.17%
Epoch 61/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.40it/s, Loss=0.031, Acc=99.1%]
Epoch 62/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 120.08it/s, Loss=0.034, Acc=98.9%]
Epoch 63/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 128.42it/s, Loss=0.034, Acc=99.1%]
Epoch 64/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.99it/s, Loss=0.030, Acc=99.0%]
Epoch 65/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.55it/s, Loss=0.030, Acc=99.1%]
Epoch 66/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.59it/s, Loss=0.029, Acc=99.1%]
Epoch 67/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.79it/s, Loss=0.028, Acc=99.1%]
Epoch 68/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 128.13it/s, Loss=0.029, Acc=99.1%]
Epoch 69/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.38it/s, Loss=0.027, Acc=99.1%]
Epoch 70/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.42it/s, Loss=0.028, Acc=99.1%]
  Epoch 70: Train 99.13% | Test 27.84% | Gap 71.29%
Epoch 71/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.42it/s, Loss=0.026, Acc=99.1%]
Epoch 72/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 126.93it/s, Loss=0.028, Acc=99.2%]
Epoch 73/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 133.85it/s, Loss=0.025, Acc=99.2%]
Epoch 74/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 141.10it/s, Loss=0.023, Acc=99.2%]
Epoch 75/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.19it/s, Loss=0.025, Acc=99.2%]
Epoch 76/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.81it/s, Loss=0.025, Acc=99.2%]
Epoch 77/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 127.62it/s, Loss=0.026, Acc=99.2%]
Epoch 78/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 130.10it/s, Loss=0.025, Acc=99.2%]
Epoch 79/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.05it/s, Loss=0.025, Acc=99.2%]
Epoch 80/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 136.54it/s, Loss=0.024, Acc=99.2%]
  Epoch 80: Train 99.20% | Test 27.86% | Gap 71.34%
Epoch 81/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 139.44it/s, Loss=0.023, Acc=99.2%]
Epoch 82/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.28it/s, Loss=0.025, Acc=99.3%]
Epoch 83/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 129.92it/s, Loss=0.025, Acc=99.2%]
Epoch 84/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.59it/s, Loss=0.024, Acc=99.3%]
Epoch 85/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 139.19it/s, Loss=0.023, Acc=99.2%]
Epoch 86/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 136.67it/s, Loss=0.023, Acc=99.2%]
Epoch 87/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.83it/s, Loss=0.023, Acc=99.3%]
Epoch 88/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 137.51it/s, Loss=0.023, Acc=99.2%]
Epoch 89/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 123.65it/s, Loss=0.025, Acc=99.3%]
Epoch 90/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 132.75it/s, Loss=0.024, Acc=99.3%]
  Epoch 90: Train 99.27% | Test 27.92% | Gap 71.35%
Epoch 91/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.87it/s, Loss=0.023, Acc=99.2%]
Epoch 92/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.38it/s, Loss=0.024, Acc=99.2%]
Epoch 93/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 126.81it/s, Loss=0.025, Acc=99.3%]
Epoch 94/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 134.95it/s, Loss=0.023, Acc=99.3%]
Epoch 95/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 139.43it/s, Loss=0.023, Acc=99.3%]
Epoch 96/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 140.87it/s, Loss=0.022, Acc=99.3%]
Epoch 97/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 141.94it/s, Loss=0.022, Acc=99.3%]
Epoch 98/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 135.98it/s, Loss=0.024, Acc=99.3%]
Epoch 99/120: 100%|█████████████████████████| 47/47 [00:00<00:00, 126.47it/s, Loss=0.025, Acc=99.3%]
Epoch 100/120: 100%|████████████████████████| 47/47 [00:00<00:00, 129.24it/s, Loss=0.024, Acc=99.3%]
  Epoch 100: Train 99.27% | Test 27.80% | Gap 71.47%
Epoch 101/120: 100%|████████████████████████| 47/47 [00:00<00:00, 133.46it/s, Loss=0.023, Acc=99.3%]
Epoch 102/120: 100%|████████████████████████| 47/47 [00:00<00:00, 134.12it/s, Loss=0.024, Acc=99.3%]
Epoch 103/120: 100%|████████████████████████| 47/47 [00:00<00:00, 138.21it/s, Loss=0.023, Acc=99.3%]
Epoch 104/120: 100%|████████████████████████| 47/47 [00:00<00:00, 134.23it/s, Loss=0.023, Acc=99.3%]
Epoch 105/120: 100%|████████████████████████| 47/47 [00:00<00:00, 131.48it/s, Loss=0.024, Acc=99.3%]
Epoch 106/120: 100%|████████████████████████| 47/47 [00:00<00:00, 134.03it/s, Loss=0.023, Acc=99.3%]
Epoch 107/120: 100%|████████████████████████| 47/47 [00:00<00:00, 133.91it/s, Loss=0.023, Acc=99.3%]
Epoch 108/120: 100%|████████████████████████| 47/47 [00:00<00:00, 128.15it/s, Loss=0.025, Acc=99.3%]
Epoch 109/120: 100%|████████████████████████| 47/47 [00:00<00:00, 127.74it/s, Loss=0.024, Acc=99.3%]
Epoch 110/120: 100%|████████████████████████| 47/47 [00:00<00:00, 133.96it/s, Loss=0.023, Acc=99.3%]
  Epoch 110: Train 99.27% | Test 27.82% | Gap 71.45%
Epoch 111/120: 100%|████████████████████████| 47/47 [00:00<00:00, 128.81it/s, Loss=0.023, Acc=99.3%]
Epoch 112/120: 100%|████████████████████████| 47/47 [00:00<00:00, 126.08it/s, Loss=0.024, Acc=99.3%]
Epoch 113/120: 100%|████████████████████████| 47/47 [00:00<00:00, 129.38it/s, Loss=0.023, Acc=99.3%]
Epoch 114/120: 100%|████████████████████████| 47/47 [00:00<00:00, 131.88it/s, Loss=0.023, Acc=99.3%]
Epoch 115/120: 100%|████████████████████████| 47/47 [00:00<00:00, 134.64it/s, Loss=0.024, Acc=99.3%]
Epoch 116/120: 100%|████████████████████████| 47/47 [00:00<00:00, 137.06it/s, Loss=0.022, Acc=99.3%]
Epoch 117/120: 100%|████████████████████████| 47/47 [00:00<00:00, 139.19it/s, Loss=0.022, Acc=99.3%]
Epoch 118/120: 100%|████████████████████████| 47/47 [00:00<00:00, 138.69it/s, Loss=0.022, Acc=99.3%]
Epoch 119/120: 100%|████████████████████████| 47/47 [00:00<00:00, 129.92it/s, Loss=0.023, Acc=99.3%]
Epoch 120/120: 100%|████████████████████████| 47/47 [00:00<00:00, 131.84it/s, Loss=0.023, Acc=99.3%]
  Epoch 120: Train 99.27% | Test 27.91% | Gap 71.36%
✓ Saved to checkpoints/shadow/shadow_3.pth

============================================================
✓ All shadow models trained!
============================================================

Next step: Run train_attack.py to train the attack model
PS C:\Users\jaiog\OneDrive\rebecca uni\student_kit> python train_attack.py
>>
Using device: cuda
============================================================
Training Membership Inference Attack Model (Baseline)
============================================================

[Shadow 1/3] Extracting features...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\models\model.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
  → Training data (IN)...
Extracting features: 100%|████████████████████| 500/500 [00:05<00:00, 89.86it/s]
  → Test data (OUT)...
Extracting features: 100%|████████████████████| 100/100 [00:01<00:00, 95.78it/s]

[Shadow 2/3] Extracting features...
  → Training data (IN)...
Extracting features: 100%|████████████████████| 500/500 [00:06<00:00, 79.47it/s]
  → Test data (OUT)...
Extracting features: 100%|████████████████████| 100/100 [00:01<00:00, 70.07it/s]

[Shadow 3/3] Extracting features...
  → Training data (IN)...
Extracting features: 100%|████████████████████| 500/500 [00:07<00:00, 67.02it/s]
  → Test data (OUT)...
Extracting features: 100%|████████████████████| 100/100 [00:01<00:00, 65.77it/s]

Original data distribution:
  IN samples: 150000
  OUT samples: 30000
  Ratio (IN:OUT): 5.00:1

============================================================
Training Attack Model (Baseline: 20 epochs)
============================================================
Epoch  5: Train Acc: 83.33%, Test Acc: 83.33% ← BEST
Epoch 10: Train Acc: 83.33%, Test Acc: 83.33% ← BEST
Epoch 15: Train Acc: 83.33%, Test Acc: 83.33% ← BEST
Epoch 20: Train Acc: 83.33%, Test Acc: 83.33% ← BEST

============================================================
✓ Baseline Attack Model Saved!
============================================================
Best Accuracy: 83.33%

✅ SUCCESS! Attack accuracy: 83.33% (>65%)

Next step: Run evaluate.py to test on target model
PS C:\Users\jaiog\OneDrive\rebecca uni\student_kit> python evaluate.py
>>
============================================================
Evaluating Membership Inference Attack
============================================================

Loading target model...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\models\model.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
Loading attack model...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\evaluate.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load('checkpoints/attack/attack_model.pth', map_location=device)
Loading test samples...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\evaluate.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_data = torch.load('test_samples.pth')
  IN samples: 100
  OUT samples: 100

Extracting features...
Running attack...

============================================================
RESULTS
============================================================

IN samples (should predict 'member'):
  Correct: 100/100
  Accuracy: 100.00%

OUT samples (should predict 'non-member'):
  Correct: 0/100
  Accuracy: 0.00%

Overall Attack Accuracy: 50.00%
============================================================

❌ Attack failed (<55%)

Feature Statistics:
  IN  - Loss: 0.0031, Correctness: 1.00
  OUT - Loss: 4.9115, Correctness: 0.29
  Loss Gap: 4.9084
PS C:\Users\jaiog\OneDrive\rebecca uni\student_kit> python train_attack.py
>>
Using device: cuda
============================================================
Training Membership Inference Attack Model (Baseline)
============================================================

[Shadow 1/3] Extracting features...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\models\model.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
  → Training data (IN)...
Extracting features: 100%|████████████████████| 500/500 [00:05<00:00, 86.74it/s]
  → Test data (OUT)...
Extracting features: 100%|████████████████████| 100/100 [00:01<00:00, 93.13it/s]

[Shadow 2/3] Extracting features...
  → Training data (IN)...
Extracting features: 100%|████████████████████| 500/500 [00:06<00:00, 78.62it/s]
  → Test data (OUT)...
Extracting features: 100%|████████████████████| 100/100 [00:01<00:00, 68.71it/s]

[Shadow 3/3] Extracting features...
  → Training data (IN)...
Extracting features: 100%|████████████████████| 500/500 [00:07<00:00, 66.23it/s]
  → Test data (OUT)...
Extracting features: 100%|████████████████████| 100/100 [00:01<00:00, 66.35it/s]

Original data distribution:
  IN samples: 150000
  OUT samples: 30000
  Ratio (IN:OUT): 5.00:1

Balanced dataset:
  IN samples: 30000
  OUT samples: 30000

============================================================
Training Attack Model (Baseline: 20 epochs)
============================================================
Epoch  5: Train Acc: 50.71%, Test Acc: 51.58% ← BEST
Epoch 10: Train Acc: 50.80%, Test Acc: 51.59%
Epoch 15: Train Acc: 50.85%, Test Acc: 51.58%
Epoch 20: Train Acc: 50.99%, Test Acc: 51.60%

============================================================
✓ Baseline Attack Model Saved!
============================================================
Best Accuracy: 51.76%

⚠️  HINT: Your accuracy is very low (<55%)
   → Check if you're using the CORRECT normalization
   → Shadow models might not be trained well

Next step: Run evaluate.py to test on target model
PS C:\Users\jaiog\OneDrive\rebecca uni\student_kit> python evaluate.py
>>
============================================================
Evaluating Membership Inference Attack
============================================================

Loading target model...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\models\model.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
Loading attack model...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\evaluate.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load('checkpoints/attack/attack_model.pth', map_location=device)
Loading test samples...
C:\Users\jaiog\OneDrive\rebecca uni\student_kit\evaluate.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_data = torch.load('test_samples.pth')
  IN samples: 100
  OUT samples: 100

Extracting features...
Running attack...

============================================================
RESULTS
============================================================

IN samples (should predict 'member'):
  Correct: 100/100
  Accuracy: 100.00%

OUT samples (should predict 'non-member'):
  Correct: 76/100
  Accuracy: 76.00%

Overall Attack Accuracy: 88.00%
============================================================

✓ ATTACK SUCCESSFUL! (>65%)

Feature Statistics:
  IN  - Loss: 0.0031, Correctness: 1.00
  OUT - Loss: 4.9115, Correctness: 0.29
  Loss Gap: 4.9084